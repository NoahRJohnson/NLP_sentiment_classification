{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "**Build a naive bayes sentiment classifier using add-1 smoothing, as described in the lecture notes (not binary naive bayes, regular naive bayes). Add an unknown word UNK, as a separate word with count 0. Here is our training corpus:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "negatives = [\"just plain boring\",\n",
    "             \"entirely predictable and lacks energy\",\n",
    "             \"no surprises and very few laughs\"]\n",
    "positives = [\"very powerful\",\n",
    "             \"the most fun film of the summer\"]\n",
    "documents = {'negative': negatives,\n",
    "             'positive': positives}\n",
    "classes = documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "test_sentence = \"predictable with no originality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unknown word token\n",
    "UNK = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the prior for the two classes + and -, and the likelihoods for each word given the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Helper function to tokenize a string. I made\n",
    "    this a function so we can swap out our tokenization\n",
    "    function in the future if we want to play around with\n",
    "    things.\n",
    "    \"\"\"\n",
    "    tokens = document.lower().split()  # lowercase, split on whitespace\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(w, c, D):\n",
    "    \"\"\"\n",
    "    helper function to count occurrences\n",
    "    of token w in documents D of class c\n",
    "    \n",
    "    uses add-1 smoothing\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        n += tokens.count(w)  # list-counting method\n",
    "    return (n + 1)  # add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.6, 'positive': 0.4}\n",
      "{('film', 'positive'): 0.06666666666666667, ('the', 'negative'): 0.02857142857142857, ('surprises', 'positive'): 0.03333333333333333, ('UNK', 'negative'): 0.02857142857142857, ('of', 'positive'): 0.06666666666666667, ('powerful', 'negative'): 0.02857142857142857, ('laughs', 'positive'): 0.03333333333333333, ('no', 'positive'): 0.03333333333333333, ('most', 'positive'): 0.06666666666666667, ('predictable', 'positive'): 0.03333333333333333, ('boring', 'positive'): 0.03333333333333333, ('just', 'negative'): 0.05714285714285714, ('predictable', 'negative'): 0.05714285714285714, ('energy', 'negative'): 0.05714285714285714, ('plain', 'positive'): 0.03333333333333333, ('fun', 'positive'): 0.06666666666666667, ('lacks', 'negative'): 0.05714285714285714, ('lacks', 'positive'): 0.03333333333333333, ('summer', 'positive'): 0.06666666666666667, ('few', 'positive'): 0.03333333333333333, ('few', 'negative'): 0.05714285714285714, ('energy', 'positive'): 0.03333333333333333, ('boring', 'negative'): 0.05714285714285714, ('summer', 'negative'): 0.02857142857142857, ('laughs', 'negative'): 0.05714285714285714, ('plain', 'negative'): 0.05714285714285714, ('entirely', 'negative'): 0.05714285714285714, ('fun', 'negative'): 0.02857142857142857, ('powerful', 'positive'): 0.06666666666666667, ('just', 'positive'): 0.03333333333333333, ('no', 'negative'): 0.05714285714285714, ('and', 'positive'): 0.03333333333333333, ('the', 'positive'): 0.1, ('very', 'positive'): 0.06666666666666667, ('most', 'negative'): 0.02857142857142857, ('and', 'negative'): 0.08571428571428572, ('very', 'negative'): 0.05714285714285714, ('of', 'negative'): 0.02857142857142857, ('film', 'negative'): 0.02857142857142857, ('UNK', 'positive'): 0.03333333333333333, ('entirely', 'positive'): 0.03333333333333333, ('surprises', 'negative'): 0.05714285714285714}\n"
     ]
    }
   ],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    \"\"\"\n",
    "    train a naive bayes model. \n",
    "    The conditional probability of each token\n",
    "    in the vocabulary of our training set is\n",
    "    calculated, as well as priors\n",
    "    \"\"\"\n",
    "    global tokenize, count\n",
    "    \n",
    "    # Find vocabulary - unique tokens in training set\n",
    "    vocabulary = set()\n",
    "    for c in classes:\n",
    "        for d in documents[c]:\n",
    "            tokens = tokenize(d)\n",
    "            for token in tokens:\n",
    "                vocabulary.add(token)\n",
    "\n",
    "    vocabulary.add(UNK)  # unknown word, will always have 0 count\n",
    "    \n",
    "    priors = dict()\n",
    "    likelihoods = dict()\n",
    "    \n",
    "    # Count total number of documents in training set\n",
    "    N_doc = 0\n",
    "    for c in classes:\n",
    "        N_doc += len(documents[c])\n",
    "\n",
    "    # Compute priors and likelihoods\n",
    "    for c in C:\n",
    "\n",
    "        # Compute prior for this class\n",
    "        c_documents_list = D[c]\n",
    "        N_c = len(c_documents_list)\n",
    "        priors[c] = N_c / N_doc\n",
    "        \n",
    "        # sum of count(w,c) for all words in the vocabulary\n",
    "        c_vocab_total_count = sum([count(w,c,D) for w in vocabulary])\n",
    "        \n",
    "\n",
    "        for token in vocabulary:\n",
    "            likelihoods[(token, c)] = count(token, c, D) / c_vocab_total_count\n",
    "            \n",
    "    return (priors, likelihoods, vocabulary)\n",
    "\n",
    "priors, liks, vocab = train_naive_bayes(documents, classes)\n",
    "print(priors)\n",
    "print(liks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then compute whether the sentence in the test set is of class positive or negative. Make sure you know the correct Bayes equation to use to compute a value for each class in order to answer this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 1.599333610995418e-06, 'positive': 4.938271604938272e-07}\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "def test_naive_bayes(test_sent, priors, liks, vocabulary, C):\n",
    "    \"\"\"\n",
    "    test_tokens is a list of tokens\n",
    "    preprocessed (for UNK replacement etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace unknown test set words with UNK token\n",
    "    test_tokens = tokenize(test_sent)\n",
    "\n",
    "    for i in range(len(test_tokens)):\n",
    "        if test_tokens[i] not in vocabulary:\n",
    "            test_tokens[i] = UNK\n",
    "    \n",
    "    # Compute class probabilities for this string of tokens\n",
    "    class_probabilities = dict()\n",
    "    \n",
    "    for c in C:\n",
    "        class_probabilities[c] = priors[c]\n",
    "\n",
    "        for token in test_tokens:\n",
    "            class_probabilities[c] *= liks[(token, c)]\n",
    "            \n",
    "    print(class_probabilities)\n",
    "    return max(class_probabilities, key=class_probabilities.get)  # argmax\n",
    "\n",
    "print(test_naive_bayes(test_sentence, priors, liks, vocab, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would the answer be without add-1 smoothing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.0, 'positive': 0.0}\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "def count(w, c, D):\n",
    "    \"\"\"\n",
    "    the power of modularity.\n",
    "    redefine our helper count function,\n",
    "    to do the same thing but not use add-1\n",
    "    smoothing.\n",
    "    \n",
    "    count occurrences\n",
    "    of token w in class c\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        n += tokens.count(w)  # list-counting method\n",
    "    return n\n",
    "\n",
    "priors, liks, vocab = train_naive_bayes(documents, classes)\n",
    "print(test_naive_bayes(test_sentence, priors, liks, vocab, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without add-1 smoothing, the unknown words make the probability for any class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Would using binary multinomial Naive Bayes change anything?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because changing the counts to 0 would still leave the unknown words in the test set with a count of 0, and thus a conditional probability of 0. We either need to use add-1 smoothing, or drop the unknown words entirely from the test sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would happen if you used the second alternative method in Section 3.3.1 of J&M to determine the count of UNK?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method mentioned is to set some amount of words with occur infrequently in the training set, to UNK, and then compute the count of UNK normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': 1, 'lacks': 1, 'the': 2, 'boring': 1, 'fun': 1, 'and': 2, 'powerful': 1, 'no': 1, 'summer': 1, 'film': 1, 'laughs': 1, 'surprises': 1, 'very': 2, 'entirely': 1, 'of': 1, 'few': 1, 'plain': 1, 'most': 1, 'just': 1, 'predictable': 1}\n"
     ]
    }
   ],
   "source": [
    "# count words\n",
    "token_counts = {}\n",
    "for c in classes:\n",
    "    for d in documents[c]:\n",
    "        for token in tokenize(d):\n",
    "            token_counts[token] = token_counts.get(token, 0) + 1\n",
    "            \n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, most words occur infrequently in our training set. If we set tokens with count less than 2 to UNK, we would end up removing most of the useful information from our model. So we won't do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "**We are given the following corpus, modified from the one in the chapter:**\n",
    "\n",
    "**&lt;s&gt; I am Sam &lt;/s&gt;**\n",
    "\n",
    "**&lt;s&gt; Sam I am &lt;/s&gt;**\n",
    "\n",
    "**&lt;s&gt; I am Sam &lt;/s&gt;**\n",
    "\n",
    "**&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;**\n",
    "\n",
    "**If we use linear interpolation smoothing between a maximum-likelihood bi-gram model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(Sam \\mid am)$? Include &lt;s&gt; and &lt;/s&gt; in your counts just like any other token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model:  {'i': 0.16, 'like': 0.04, 'not': 0.04, 'am': 0.12, 'green': 0.04, 'sam': 0.16, '<s>': 0.16, 'do': 0.04, 'and': 0.04, 'eggs': 0.04, '</s>': 0.16}\n",
      "Bigram model:  {('<s>', 'i'): 0.75, ('am', 'sam'): 0.6666666666666666, ('sam', 'i'): 0.25, ('like', 'green'): 1.0, ('sam', '</s>'): 0.75, ('do', 'not'): 1.0, ('not', 'like'): 1.0, ('am', '</s>'): 0.3333333333333333, ('i', 'do'): 0.25, ('eggs', 'and'): 1.0, ('and', 'sam'): 1.0, ('i', 'am'): 0.75, ('green', 'eggs'): 1.0, ('<s>', 'sam'): 0.25}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize training corpus\n",
    "training_sentences = ['<s> I am Sam </s>',\n",
    "                      '<s> Sam I am </s>',\n",
    "                      '<s> I am Sam </s>',\n",
    "                      '<s> I do not like green eggs and Sam </s>']\n",
    "\n",
    "# Let's use a basic split for our tokenization in this case, because we want <s> to be one token.\n",
    "\n",
    "\n",
    "training_tokens = [tokenize(sent) for sent in training_sentences]\n",
    "\n",
    "# Build unigram and bigram counts\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for token_list in training_tokens:\n",
    "    unigram_counts += Counter(token_list)\n",
    "    bigram_counts += Counter(zip(token_list[:-1], token_list[1:]))\n",
    "\n",
    "# Normalize counts to get MLE models\n",
    "unigram_model = dict(unigram_counts)\n",
    "bigram_model = dict(bigram_counts)\n",
    "\n",
    "unigram_total_count = sum(unigram_model.values())  # sum all unigram counts\n",
    "\n",
    "for unigram in unigram_model.keys():\n",
    "    unigram_model[unigram] /= unigram_total_count\n",
    "    \n",
    "for bigram in bigram_model.keys():\n",
    "    w1 = bigram[0]\n",
    "    bigram_model[bigram] /= unigram_counts[w1]\n",
    "\n",
    "# These models are max likelihood estimators, so they aren't necessarily PDFs\n",
    "# in this case the unigram will be a PDF but the bigram estimate won't be\n",
    "print(\"Unigram model: \", unigram_model)\n",
    "print(\"Bigram model: \", bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Sam | am) = 0.413333\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 1/2\n",
    "lambda2 = 1/2\n",
    "\n",
    "p_am_sam = lambda1 * unigram_model['sam'] + lambda2 * bigram_model[('am', 'sam')]\n",
    "print(\"P(Sam | am) = %f\" % p_am_sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**Suppose we didn’t use the end-symbol &lt;/s&gt;. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol &lt;/s&gt;:**\n",
    "\n",
    "**&lt;s&gt; a b**\n",
    "\n",
    "**&lt;s&gt; b b**\n",
    "\n",
    "**&lt;s&gt; b a**\n",
    "\n",
    "**&lt;s&gt; a a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model:  {('a', 'b'): 0.25, ('b', 'b'): 0.25, ('b', 'a'): 0.25, ('<s>', 'a'): 0.5, ('a', 'a'): 0.25, ('<s>', 'b'): 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training corpus\n",
    "training_sentences = ['<s> a b',\n",
    "                      '<s> b b',\n",
    "                      '<s> b a',\n",
    "                      '<s> a a']\n",
    "\n",
    "training_tokens = [tokenize(sent) for sent in training_sentences]\n",
    "\n",
    "# Build unigram and bigram counts\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for token_list in training_tokens:\n",
    "    unigram_counts += Counter(token_list)\n",
    "    bigram_counts += Counter(zip(token_list[:-1], token_list[1:]))\n",
    "\n",
    "# Normalize bigram counts to get MLE model\n",
    "bigram_model = dict(bigram_counts)\n",
    "    \n",
    "for bigram in bigram_model.keys():\n",
    "    w1 = bigram[0]\n",
    "    bigram_model[bigram] /= unigram_counts[w1]\n",
    "\n",
    "print(\"Bigram model: \", bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet {a,b} is also 1.0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'a') : 0.25\n",
      "('a', 'b') : 0.25\n",
      "('b', 'a') : 0.25\n",
      "('b', 'b') : 0.25\n",
      "Sum: 1.00\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "alphabet = ['a', 'b']\n",
    "\n",
    "# produce cartesian product of alphabet with itself\n",
    "possible_two_word_sentences = itertools.product(alphabet, repeat=2)\n",
    "\n",
    "prob_sum = 0\n",
    "for sent in possible_two_word_sentences:\n",
    "    sent_prob = bigram_model[sent]\n",
    "\n",
    "    prob_sum += sent_prob\n",
    "    print(sent, \":\", sent_prob)\n",
    "    \n",
    "print(\"Sum: %.2f\" % prob_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'a', 'a') : 0.0625\n",
      "('a', 'a', 'b') : 0.0625\n",
      "('a', 'b', 'a') : 0.0625\n",
      "('a', 'b', 'b') : 0.0625\n",
      "('b', 'a', 'a') : 0.0625\n",
      "('b', 'a', 'b') : 0.0625\n",
      "('b', 'b', 'a') : 0.0625\n",
      "('b', 'b', 'b') : 0.0625\n",
      "Sum: 0.50\n"
     ]
    }
   ],
   "source": [
    "possible_three_word_sentences = itertools.product(alphabet, repeat=3)\n",
    "\n",
    "prob_sum = 0\n",
    "for sent in possible_three_word_sentences:\n",
    "    bigram1 = sent[:2]\n",
    "    bigram2 = sent[1:]\n",
    "    \n",
    "    sent_prob = bigram_model[bigram1] * bigram_model[bigram2]\n",
    "\n",
    "    prob_sum += sent_prob\n",
    "    print(sent, \":\", sent_prob)\n",
    "    \n",
    "print(\"Sum: %.2f\" % prob_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second sum is not 1.0, but the point is still made since obviously the sum of probabilities over all sentence lengths will be > 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "**A robot, which only has a camera as a sensor, can either be in one of two locations: L1 or L2. The robot doesn’t know exactly where it is and it represents this uncertainty by keeping track of two probabilities: P(L1) and P(L2). Based on all past observations, the robot thinks that there is a 0.8 probability it is in L1 and a 0.2 probability that it is in L2.**\n",
    "\n",
    "**The robot’s vision algorithm detects a window, and although there is only a window in L2, it can’t conclude that it is in fact in L2: its image recognition algorithm is not perfect. The probability of observing a window given there is no window at its location is 0.2 and the probability of observing a window given there is a window is 0.9. After incorporating the observation of a window, what is the robot’s new values for P(L1) and P(L2)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes Rule, $$P(LX \\mid window) = \\frac{P(window \\mid LX) * P(LX)}{P(window)}$$\n",
    "\n",
    "We can use the law of total probability: $$P(Y) = \\sum_\\limits{X \\in \\Omega} P(Y | X) * P(X)$$ to find $P(window)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(L1) = 0.470588\n",
      "P(L2) = 0.529412\n"
     ]
    }
   ],
   "source": [
    "prior_l1 = 0.8\n",
    "prior_l2 = 0.2\n",
    "\n",
    "p_window_l1 = 0.2\n",
    "p_window_l2 = 0.9\n",
    "\n",
    "# law of total probability\n",
    "p_window = p_window_l1 * prior_l1 + p_window_l2 * prior_l2\n",
    "\n",
    "# bayes rule\n",
    "posterior_l1 = p_window_l1 * prior_l1 / p_window\n",
    "posterior_l2 = p_window_l2 * prior_l2 / p_window\n",
    "\n",
    "print(\"P(L1) = %f\" % posterior_l1)\n",
    "print(\"P(L2) = %f\" % posterior_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we see how human intuition often fails to grasp bayesian statistics. At least my intuition often does ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "**Binary multinomial NB seems to work better on some problems than full count NB, but full count works better on others. For what kinds of problems might binary NB be better, and why? (There is no known right answer to this question, but I'd like you to think about the possibilities.) Come up with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary NB is less sensitive to the details in text. I'm not convinced that's a good thing, though. If an author describes a movie as \"fantastic\" 5 times in a text, then disregarding negation that seems to me stronger evidence of a positive sentiment than only one instance of \"fantastic\". I would consider using Binary NB when you have a lack of data, and want a simpler model to ensure it will generalize. Let's look at an example below of where the two models can disagree given the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full count with add-1 smoothing\n",
    "def count(w, c, D):\n",
    "    n = 0\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        n += tokens.count(w)\n",
    "    return (n + 1)  # add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors:  {'negative': 0.6, 'positive': 0.4}\n",
      "Likelihoods:  {('great', 'positive'): 0.2857142857142857, ('good', 'positive'): 0.2857142857142857, ('UNK', 'negative'): 0.14285714285714285, ('poor', 'negative'): 0.2857142857142857, ('good', 'negative'): 0.2857142857142857, ('UNK', 'positive'): 0.14285714285714285, ('poor', 'positive'): 0.2857142857142857, ('great', 'negative'): 0.2857142857142857}\n"
     ]
    }
   ],
   "source": [
    "# This example is inspired by Exercise 4.3\n",
    "negatives = [\"Good poor poor poor\",\n",
    "             \"poor great good poor \" + \\\n",
    "             \"great poor poor POOR\",\n",
    "             \"Poor poor\"]\n",
    "\n",
    "positives = [\"good good good Great great great\",\n",
    "             \"poor great great\"]\n",
    "\n",
    "documents = {'negative': negatives,\n",
    "             'positive': positives}\n",
    "classes = documents.keys()\n",
    "\n",
    "priors, liks, vocab = train_naive_bayes(documents, classes)\n",
    "print(\"Priors: \", priors)\n",
    "print(\"Likelihoods: \", liks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 8.557859130511291e-15, 'positive': 2.5356986536167335e-13}\n",
      "Full count results:  positive\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"A good, good plot and great characters, but poor acting.\"\n",
    "\n",
    "print(\"Full count results: \", test_naive_bayes(test_sent, priors, liks, vocab, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods:  {('great', 'positive'): 0.2857142857142857, ('good', 'positive'): 0.2857142857142857, ('UNK', 'negative'): 0.14285714285714285, ('poor', 'negative'): 0.2857142857142857, ('good', 'negative'): 0.2857142857142857, ('UNK', 'positive'): 0.14285714285714285, ('poor', 'positive'): 0.2857142857142857, ('great', 'negative'): 0.2857142857142857}\n"
     ]
    }
   ],
   "source": [
    "# Binary count with add-1 smoothing\n",
    "def count(w, c, D):\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        if w in tokens:\n",
    "            return 2\n",
    "    return 1\n",
    "\n",
    "priors, liks, vocab = train_naive_bayes(documents, classes)\n",
    "print(\"Likelihoods: \", liks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 9.908244453806926e-11, 'positive': 6.60549630253795e-11}\n",
      "Binary count results:  negative\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary count results: \", test_naive_bayes(test_sent, priors, liks, vocab, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, full-count NB and binary NB disagree on the sentiment classification. Does it make sense for repeated adjectives like \"good, good\" to be considered as valuable information showing emphasis? Perhaps. But in this case ignoring the emphasis allowed the model to side more strongly with its priors and make the correct classification.\n",
    "\n",
    "Personally though I wouldn't worry about edge cases like this, and would trust full-count NB more given enough data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
