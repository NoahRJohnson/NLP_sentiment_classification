{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "**Build a naive bayes sentiment classifier using add-1 smoothing, as described in the lecture notes (not binary naive bayes, regular naive bayes). Add an unknown word UNK, as a separate word with count 0. Here is our training corpus:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "negatives = [\"just plain boring\",\n",
    "             \"entirely predictable and lacks energy\",\n",
    "             \"no surprises and very few laughs\"]\n",
    "positives = [\"very powerful\",\n",
    "             \"the most fun film of the summer\"]\n",
    "documents = {'negative': negatives,\n",
    "             'positive': positives}\n",
    "classes = documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "test_sentence = \"predictable with no originality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the prior for the two classes + and -, and the likelihoods for each word given the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Helper function to tokenize a string. For now we're just splitting, but I made\n",
    "    this a function so we can swap out to a nltk function in the future easily if\n",
    "    we want to.\n",
    "    \"\"\"\n",
    "    tokens = document.split()  # basic tokenization\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plain', 'fun', 'the', 'of', 'predictable', 'very', 'summer', 'and', 'energy', 'powerful', 'boring', 'laughs', 'UNK', 'no', 'surprises', 'film', 'lacks', 'most', 'entirely', 'few', 'just'}\n"
     ]
    }
   ],
   "source": [
    "# Find vocabulary - unique tokens in training set\n",
    "vocabulary = set()\n",
    "for c in classes:\n",
    "    for d in documents[c]:\n",
    "        tokens = tokenize(d)\n",
    "        for token in tokens:\n",
    "            vocabulary.add(token)\n",
    "            \n",
    "vocabulary.add(UNK)  # unknown word, will always have 0 count\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(w, c, D):\n",
    "    \"\"\"\n",
    "    helper function to count occurrences\n",
    "    of token w in documents D of class c\n",
    "    \n",
    "    uses add-1 smoothing\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        n += tokens.count(w)  # list-counting method\n",
    "    return (n + 1)  # add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.4, 'negative': 0.6}\n",
      "{('powerful', 'negative'): 0.02857142857142857, ('surprises', 'negative'): 0.05714285714285714, ('plain', 'negative'): 0.05714285714285714, ('very', 'negative'): 0.05714285714285714, ('just', 'positive'): 0.03333333333333333, ('few', 'negative'): 0.05714285714285714, ('few', 'positive'): 0.03333333333333333, ('boring', 'negative'): 0.05714285714285714, ('the', 'positive'): 0.1, ('energy', 'negative'): 0.05714285714285714, ('just', 'negative'): 0.05714285714285714, ('energy', 'positive'): 0.03333333333333333, ('plain', 'positive'): 0.03333333333333333, ('boring', 'positive'): 0.03333333333333333, ('summer', 'positive'): 0.06666666666666667, ('film', 'negative'): 0.02857142857142857, ('very', 'positive'): 0.06666666666666667, ('surprises', 'positive'): 0.03333333333333333, ('UNK', 'positive'): 0.03333333333333333, ('no', 'negative'): 0.05714285714285714, ('lacks', 'positive'): 0.03333333333333333, ('entirely', 'negative'): 0.05714285714285714, ('of', 'negative'): 0.02857142857142857, ('predictable', 'positive'): 0.03333333333333333, ('of', 'positive'): 0.06666666666666667, ('most', 'positive'): 0.06666666666666667, ('and', 'positive'): 0.03333333333333333, ('laughs', 'negative'): 0.05714285714285714, ('summer', 'negative'): 0.02857142857142857, ('laughs', 'positive'): 0.03333333333333333, ('most', 'negative'): 0.02857142857142857, ('predictable', 'negative'): 0.05714285714285714, ('powerful', 'positive'): 0.06666666666666667, ('fun', 'negative'): 0.02857142857142857, ('and', 'negative'): 0.08571428571428572, ('fun', 'positive'): 0.06666666666666667, ('film', 'positive'): 0.06666666666666667, ('lacks', 'negative'): 0.05714285714285714, ('no', 'positive'): 0.03333333333333333, ('the', 'negative'): 0.02857142857142857, ('UNK', 'negative'): 0.02857142857142857, ('entirely', 'positive'): 0.03333333333333333}\n"
     ]
    }
   ],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    \"\"\"\n",
    "    train a naive bayes model. \n",
    "    The conditional probability of each token\n",
    "    in the vocabulary of our training set is\n",
    "    calculated, as well as priors\n",
    "    \"\"\"\n",
    "    global vocabulary\n",
    "    \n",
    "    priors = dict()\n",
    "    likelihoods = dict()\n",
    "    \n",
    "    # Count total number of documents in training set\n",
    "    N_doc = 0\n",
    "    for c in classes:\n",
    "        N_doc += len(documents[c])\n",
    "\n",
    "    # Compute priors and likelihoods\n",
    "    for c in C:\n",
    "\n",
    "        # Compute prior for this class\n",
    "        c_documents_list = D[c]\n",
    "        N_c = len(c_documents_list)\n",
    "        priors[c] = N_c / N_doc\n",
    "        \n",
    "        # sum of count(w,c) for all words in the vocabulary\n",
    "        c_vocab_total_count = sum([count(w,c,D) for w in vocabulary])\n",
    "        #print([count(w,c,D) for w in vocabulary])\n",
    "\n",
    "        for token in vocabulary:\n",
    "            likelihoods[(token, c)] = count(token, c, D) / c_vocab_total_count\n",
    "            \n",
    "    return (priors, likelihoods)\n",
    "\n",
    "priors, liks = train_naive_bayes(documents, classes)\n",
    "print(priors)\n",
    "print(liks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then compute whether the sentence in the test set is of class positive or negative. Make sure you know the correct Bayes equation to use to compute a value for each class in order to answer this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictable', 'with', 'no', 'originality']\n"
     ]
    }
   ],
   "source": [
    "test_sentence_tokens = tokenize(test_sentence)\n",
    "print(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictable', 'UNK', 'no', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "# replace unknown test set words with UNK token\n",
    "for i in range(len(test_sentence_tokens)):\n",
    "    if test_sentence_tokens[i] not in vocabulary:\n",
    "        test_sentence_tokens[i] = UNK\n",
    "        \n",
    "print(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 4.938271604938272e-07, 'negative': 1.599333610995418e-06}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_naive_bayes(test_tokens, priors, liks, C):\n",
    "    \"\"\"\n",
    "    test_tokens is a list of tokens\n",
    "    preprocessed (for UNK replacement etc.)\n",
    "    \"\"\"\n",
    "    global vocabulary\n",
    "    \n",
    "    class_probabilities = dict()\n",
    "    \n",
    "    for c in C:\n",
    "        class_probabilities[c] = priors[c]\n",
    "\n",
    "        for token in test_tokens:\n",
    "            class_probabilities[c] *= liks[(token, c)]\n",
    "            \n",
    "    print(class_probabilities)\n",
    "    return max(class_probabilities, key=class_probabilities.get)  # argmax\n",
    "\n",
    "test_naive_bayes(test_sentence_tokens, priors, liks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would the answer be without add-1 smoothing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.0, 'negative': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(w, c, D):\n",
    "    \"\"\"\n",
    "    the power of modularity.\n",
    "    redefine our helper count function,\n",
    "    to do the same thing but not use add-1\n",
    "    smoothing.\n",
    "    \n",
    "    count occurrences\n",
    "    of token w in class c\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for d in D[c]:\n",
    "        tokens = tokenize(d)\n",
    "        n += tokens.count(w)  # list-counting method\n",
    "    return n\n",
    "\n",
    "priors, liks = train_naive_bayes(documents, classes)\n",
    "test_naive_bayes(test_sentence_tokens, priors, liks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without add-1 smoothing, the unknown words make the probability for any class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Would using binary multinomial Naive Bayes change anything?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because changing the counts to 0 would still leave the unknown words in the test set with a count of 0, and thus a conditional probability of 0. We either need to use add-1 smoothing, or drop the unknown words entirely from the test sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would happen if you used the second alternative method in Section 3.3.1 of J&M to determine the count of UNK?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method mentioned is to set some amount of words with occur infrequently in the training set, to UNK, and then compute the count of UNK normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plain': 1, 'fun': 1, 'the': 2, 'of': 1, 'summer': 1, 'boring': 1, 'energy': 1, 'no': 1, 'powerful': 1, 'very': 2, 'surprises': 1, 'film': 1, 'lacks': 1, 'most': 1, 'just': 1, 'entirely': 1, 'few': 1, 'predictable': 1, 'and': 2, 'laughs': 1}\n"
     ]
    }
   ],
   "source": [
    "# count words\n",
    "token_counts = {}\n",
    "for c in classes:\n",
    "    for d in documents[c]:\n",
    "        for token in tokenize(d):\n",
    "            token_counts[token] = token_counts.get(token, 0) + 1\n",
    "            \n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, most words occur infrequently in our training set. If we set tokens with count less than 2 to UNK, we would end up removing most of the useful information from our model. So we won't do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "We are given the following corpus, modified from the one in the chapter:\n",
    "\n",
    "&lt;s&gt; I am Sam &lt;/s&gt;\n",
    "\n",
    "&lt;s&gt; Sam I am &lt;/s&gt;\n",
    "\n",
    "&lt;s&gt; I am Sam &lt;/s&gt;\n",
    "\n",
    "&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;\n",
    "\n",
    "If we use linear interpolation smoothing between a maximum-likelihood bi-gram model and a maximum-likelihood unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(Sam \\mid am)$? Include <s> and </s> in your counts just like any other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Suppose we didn’t use the end-symbol &lt;/s&gt;. Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol &lt;/s&gt;:\n",
    "\n",
    "&lt;s&gt; a b\n",
    "\n",
    "&lt;s&gt; b b\n",
    "\n",
    "&lt;s&gt; b a\n",
    "\n",
    "&lt;s&gt; a a\n",
    "\n",
    "Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability\n",
    "of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet {a,b} is also 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "A robot, which only has a camera as a sensor, can either be in one of two locations: L1 or L2. The robot doesn’t know exactly where it is and it represents this uncertainty by keeping track\n",
    "of two probabilities: P(L1) and P(L2). Based on all past observations, the robot thinks that there is a 0.8 probability it is in L1 and a 0.2 probability that it is in L2.\n",
    "\n",
    "The robot’s vision algorithm detects a window, and although there is only a window in L2, it can’t conclude that it is in fact in L2: its image recognition algorithm is not perfect. The probability of observing a window given there is no window at its location is 0.2 and the probability of observing a window given there is a window is 0.9. After incorporating the observation of a window, what is the robot’s new values for P(L1) and P(L2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Binary multinomial NB seems to work better on some problems than full count NB, but full count works better on others. For what kinds of problems might binary NB be better, and why? (There is no known right answer to this question, but I'd like you to think about the possibilities.) Come up with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
